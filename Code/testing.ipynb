{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "DATADIR = \"C:/Users/20115/Desktop/Dataset/train\"\n",
    "CATEGORIES = [\"NORMAL\",\"Sick\"]\n",
    "\n",
    "for cat in CATEGORIES:\n",
    "    path = os.path.join(DATADIR,cat)\n",
    "    print(path)\n",
    "    for img in os.listdir(path):\n",
    "        img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
    "        plt.imshow(img_array ,cmap=\"gray\")\n",
    "        plt.show()\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x248bf5c2b08>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "IMG_SIZE = 200\n",
    "\n",
    "new_array = cv2.resize(img_array ,(IMG_SIZE,IMG_SIZE))\n",
    "plt.imshow(new_array ,cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "\n",
    "def create_training_data():\n",
    "    for category in CATEGORIES:  # do dogs and cats\n",
    "\n",
    "        path = os.path.join(DATADIR,category)  # create path to dogs and cats\n",
    "        class_num = CATEGORIES.index(category)  # get the classification  (0 or a 1). 0=dog 1=cat\n",
    "\n",
    "        for img in tqdm(os.listdir(path)):  # iterate over each image per dogs and cats\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
    "                training_data.append([new_array, class_num])  # add this to our training_data\n",
    "            except Exception as e:  # in the interest in keeping the output clean...\n",
    "                pass\n",
    "            #except OSError as e:\n",
    "            #    print(\"OSErrroBad img most likely\", e, os.path.join(path,img))\n",
    "            #except Exception as e:\n",
    "            #    print(\"general exception\", e, os.path.join(path,img))\n",
    "\n",
    "create_training_data()\n",
    "\n",
    "print(len(training_data))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "636\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for sample in training_data[1:10]:\n",
    "    print(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "y=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for features,label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "\n",
    "\n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_out = open(\"lames.pickle\",\"wb\")\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"hh.pickle\",\"wb\")\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[5.42777665e-07],\n",
       "        [7.84012182e-07],\n",
       "        [6.03086294e-07],\n",
       "        ...,\n",
       "        [4.82469035e-07],\n",
       "        [6.75456649e-06],\n",
       "        [1.07952447e-05]],\n",
       "\n",
       "       [[6.03086294e-07],\n",
       "        [5.42777665e-07],\n",
       "        [4.82469035e-07],\n",
       "        ...,\n",
       "        [4.82469035e-07],\n",
       "        [6.45302335e-06],\n",
       "        [1.04333929e-05]],\n",
       "\n",
       "       [[5.42777665e-07],\n",
       "        [5.42777665e-07],\n",
       "        [4.22160406e-07],\n",
       "        ...,\n",
       "        [1.80925888e-07],\n",
       "        [6.39271472e-06],\n",
       "        [1.00715411e-05]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[7.53857868e-06],\n",
       "        [7.17672690e-06],\n",
       "        [6.39271472e-06],\n",
       "        ...,\n",
       "        [4.82469035e-06],\n",
       "        [7.84012182e-07],\n",
       "        [2.77419695e-06]],\n",
       "\n",
       "       [[7.05610964e-06],\n",
       "        [6.81487512e-06],\n",
       "        [5.91024568e-06],\n",
       "        ...,\n",
       "        [4.94530761e-06],\n",
       "        [4.22160406e-07],\n",
       "        [2.83450558e-06]],\n",
       "\n",
       "       [[6.87518375e-06],\n",
       "        [6.81487512e-06],\n",
       "        [5.78962842e-06],\n",
       "        ...,\n",
       "        [4.46283858e-06],\n",
       "        [6.03086294e-07],\n",
       "        [2.59327106e-06]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-conv-64-nodes-0-dense-1588474623\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are passing a target array of shape (636, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-67decf916769>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m                       \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                       \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                       validation_split=0.3,callbacks=[tensorboard])\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'64x3-CN.model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Covid\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Covid\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m           distribution_strategy=strategy)\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Covid\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m         steps=steps_per_epoch)\n\u001b[0m\u001b[0;32m    517\u001b[0m     (x, y, sample_weights,\n\u001b[0;32m    518\u001b[0m      \u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Covid\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2536\u001b[0m           \u001b[1;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2537\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[1;32m-> 2538\u001b[1;33m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[0;32m   2539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2540\u001b[0m       \u001b[1;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Covid\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[1;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[0;32m    715\u001b[0m         raise ValueError('You are passing a target array of shape ' +\n\u001b[0;32m    716\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 717\u001b[1;33m                          \u001b[1;34m' while using as loss `categorical_crossentropy`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    718\u001b[0m                          \u001b[1;34m'`categorical_crossentropy` expects '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m                          \u001b[1;34m'targets to be binary matrices (1s and 0s) '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You are passing a target array of shape (636, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-067cf60f7f86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    " # REMEMBER YOU'RE PASSING A LIST OF THINGS YOU WISH TO PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
